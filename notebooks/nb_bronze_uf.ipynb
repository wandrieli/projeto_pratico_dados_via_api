{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d549d87-bc94-4197-ba9c-203f98532d8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Conexão API - Brasil - Extração de Dados\n",
    "\n",
    "Busca os dados da API Brasil para consulta de UFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd47e7dd-3271-46a2-9b0d-452f9f0ae4bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\n- requests\\n    Para as chamadas das APIs\\n- time\\n     Para pausas entre as chamadas, ajuda a evitar bloqueios\\n- ThreadPoolExecutor\\n    Para chamadas paralelas, aqui é usado para diminuir o tempo de execução na busca dos CNPJs listados\\n- pyspark.sql.functions\\n    Para manipulação de dados em SQL\\n\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\n   Faz o download de um arquivo csv de uma URL e salva em um arquivo local\\n   local_path: Caminho local onde o arquivo será salvo, será informado no arquivo principal\\n   url: URL da API em string\\n    \\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nCarrega o arquivo csv em um dataframe\\nspark: SparkSession\\nfile_path: Caminho do arquivo csv\\nsep: Separador do arquivo csv\\nheader: Se o arquivo csv tem cabeçalho\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nPadroniza os CNPJs, removendo os caracteres que não sejam números.\\nA APIBRASIL aceita somente chamada com CNPJ em números\\ndf: Dataframe com os CNPJs\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nGarante que a tabela seja removida e cria uma nova com os dados no Hive Metastore\\ndf: Dataframe com os dados\\ntabela: Nome da tabela no Hive Metastore, será informada no notebook principal\\nmodo: Modo de escrita no Hive Metastore, nesse caso (overwrite) sobrescreve se a tabela já existir\\n\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nConsulta dados de um CNPJ em uma API, nesse caso a APIBRASIL\\nCNPJ já deve estar padronizado, sem caracteres especiais, conforme listagem no notebook\\nSe sucesso, retorna as informações principais\\nSe erro, retorna o CNPJ com a informação de erro\\nFoi incluída uma \"pausa\" para evitar bloqueios, pois são muitos CNPJs\\n\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nConsulta os CNPJs listados de forma paralela\\nUsa o ThreadPoolExecutor para chamar a API de forma paralela, evitando demora no retorno dos dados\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nPipeline para consultar os CNPJs e salvar no Hive Metastore\\nPega os CNPJs distintos da tabela de origem e salva no Hive Metastore\\ntabela_origem: Tabela de origem informada no notebook principal\\ntabela_destino: Tabela de destino informada no notebook principal\\nmax_workers: Número de threads para consulta paralela\\n\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nConsulta a lista de estados do IBGE na BRasilAPI\\nRetorna a sigla e o nome de cada estado\\nSe erro, retorna o estado com a informação de erro\\n\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nPipeline completo para UFs\\nConsulta a API\\nCria o DataFrame e salva em uma tabela de destino informada no notebook principal\\n\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%run /Users/wandrieli.barbosa@compasso.com.br/Projeto_Triller_Engineer/funcoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46f9699a-2815-45d4-acf4-d3aa3f19fd2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mPySparkValueError\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-5721707583920552>, line 1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m df_ufs \u001B[38;5;241m=\u001B[39m pipeline_ufs(spark, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muf_bruto\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m      2\u001B[0m display(df_ufs)\n",
       "\n",
       "File \u001B[0;32m<command-6016659540535200>, line 3\u001B[0m, in \u001B[0;36mpipeline_ufs\u001B[0;34m(spark, tabela_destino)\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpipeline_ufs\u001B[39m(spark, tabela_destino: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m DataFrame:\n",
       "\u001B[1;32m      2\u001B[0m     dados \u001B[38;5;241m=\u001B[39m consulta_uf()\n",
       "\u001B[0;32m----> 3\u001B[0m     df \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mcreateDataFrame(dados)    \n",
       "\u001B[1;32m      4\u001B[0m     salva_dados(df, tabela_destino, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moverwrite\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m      5\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m df\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/session.py:665\u001B[0m, in \u001B[0;36mSparkSession.createDataFrame\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n",
       "\u001B[1;32m    660\u001B[0m         _num_cols \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(_cols)\n",
       "\u001B[1;32m    662\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _has_nulltype(_schema):\n",
       "\u001B[1;32m    663\u001B[0m         \u001B[38;5;66;03m# For cases like createDataFrame([(\"Alice\", None, 80.1)], schema)\u001B[39;00m\n",
       "\u001B[1;32m    664\u001B[0m         \u001B[38;5;66;03m# we can not infer the schema from the data itself.\u001B[39;00m\n",
       "\u001B[0;32m--> 665\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m PySparkValueError(\n",
       "\u001B[1;32m    666\u001B[0m             error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCANNOT_DETERMINE_TYPE\u001B[39m\u001B[38;5;124m\"\u001B[39m, message_parameters\u001B[38;5;241m=\u001B[39m{}\n",
       "\u001B[1;32m    667\u001B[0m         )\n",
       "\u001B[1;32m    669\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconnect\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconversion\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m LocalDataToArrowConversion\n",
       "\u001B[1;32m    671\u001B[0m \u001B[38;5;66;03m# Spark Connect will try its best to build the Arrow table with the\u001B[39;00m\n",
       "\u001B[1;32m    672\u001B[0m \u001B[38;5;66;03m# inferred schema in the client side, and then rename the columns and\u001B[39;00m\n",
       "\u001B[1;32m    673\u001B[0m \u001B[38;5;66;03m# cast the datatypes in the server side.\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mPySparkValueError\u001B[0m: [CANNOT_DETERMINE_TYPE] Some of types cannot be determined after inferring."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "PySparkValueError",
        "evalue": "[CANNOT_DETERMINE_TYPE] Some of types cannot be determined after inferring."
       },
       "metadata": {
        "errorSummary": "[CANNOT_DETERMINE_TYPE] Some of types cannot be determined after inferring."
       },
       "removedWidgets": [],
       "sqlProps": {
        "breakingChangeInfo": null,
        "errorClass": "CANNOT_DETERMINE_TYPE",
        "pysparkCallSite": "",
        "pysparkFragment": "",
        "pysparkSummary": null,
        "sqlState": null,
        "stackTrace": null,
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mPySparkValueError\u001B[0m                         Traceback (most recent call last)",
        "File \u001B[0;32m<command-5721707583920552>, line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m df_ufs \u001B[38;5;241m=\u001B[39m pipeline_ufs(spark, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muf_bruto\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      2\u001B[0m display(df_ufs)\n",
        "File \u001B[0;32m<command-6016659540535200>, line 3\u001B[0m, in \u001B[0;36mpipeline_ufs\u001B[0;34m(spark, tabela_destino)\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpipeline_ufs\u001B[39m(spark, tabela_destino: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m DataFrame:\n\u001B[1;32m      2\u001B[0m     dados \u001B[38;5;241m=\u001B[39m consulta_uf()\n\u001B[0;32m----> 3\u001B[0m     df \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mcreateDataFrame(dados)    \n\u001B[1;32m      4\u001B[0m     salva_dados(df, tabela_destino, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moverwrite\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      5\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m df\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/session.py:665\u001B[0m, in \u001B[0;36mSparkSession.createDataFrame\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n\u001B[1;32m    660\u001B[0m         _num_cols \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(_cols)\n\u001B[1;32m    662\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _has_nulltype(_schema):\n\u001B[1;32m    663\u001B[0m         \u001B[38;5;66;03m# For cases like createDataFrame([(\"Alice\", None, 80.1)], schema)\u001B[39;00m\n\u001B[1;32m    664\u001B[0m         \u001B[38;5;66;03m# we can not infer the schema from the data itself.\u001B[39;00m\n\u001B[0;32m--> 665\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m PySparkValueError(\n\u001B[1;32m    666\u001B[0m             error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCANNOT_DETERMINE_TYPE\u001B[39m\u001B[38;5;124m\"\u001B[39m, message_parameters\u001B[38;5;241m=\u001B[39m{}\n\u001B[1;32m    667\u001B[0m         )\n\u001B[1;32m    669\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconnect\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconversion\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m LocalDataToArrowConversion\n\u001B[1;32m    671\u001B[0m \u001B[38;5;66;03m# Spark Connect will try its best to build the Arrow table with the\u001B[39;00m\n\u001B[1;32m    672\u001B[0m \u001B[38;5;66;03m# inferred schema in the client side, and then rename the columns and\u001B[39;00m\n\u001B[1;32m    673\u001B[0m \u001B[38;5;66;03m# cast the datatypes in the server side.\u001B[39;00m\n",
        "\u001B[0;31mPySparkValueError\u001B[0m: [CANNOT_DETERMINE_TYPE] Some of types cannot be determined after inferring."
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_ufs = pipeline_ufs(spark, \"uf_bruto\") #erro\n",
    "display(df_ufs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fafb71cf-b945-4b9d-9618-a3bc46028cb6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>sigla</th><th>nome</th></tr></thead><tbody><tr><td>RO</td><td>Rondônia</td></tr><tr><td>AC</td><td>Acre</td></tr><tr><td>AM</td><td>Amazonas</td></tr><tr><td>RR</td><td>Roraima</td></tr><tr><td>PA</td><td>Pará</td></tr><tr><td>AP</td><td>Amapá</td></tr><tr><td>TO</td><td>Tocantins</td></tr><tr><td>MA</td><td>Maranhão</td></tr><tr><td>PI</td><td>Piauí</td></tr><tr><td>CE</td><td>Ceará</td></tr><tr><td>RN</td><td>Rio Grande do Norte</td></tr><tr><td>PB</td><td>Paraíba</td></tr><tr><td>PE</td><td>Pernambuco</td></tr><tr><td>AL</td><td>Alagoas</td></tr><tr><td>SE</td><td>Sergipe</td></tr><tr><td>BA</td><td>Bahia</td></tr><tr><td>MG</td><td>Minas Gerais</td></tr><tr><td>ES</td><td>Espírito Santo</td></tr><tr><td>RJ</td><td>Rio de Janeiro</td></tr><tr><td>SP</td><td>São Paulo</td></tr><tr><td>PR</td><td>Paraná</td></tr><tr><td>SC</td><td>Santa Catarina</td></tr><tr><td>RS</td><td>Rio Grande do Sul</td></tr><tr><td>MS</td><td>Mato Grosso do Sul</td></tr><tr><td>MT</td><td>Mato Grosso</td></tr><tr><td>GO</td><td>Goiás</td></tr><tr><td>DF</td><td>Distrito Federal</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "RO",
         "Rondônia"
        ],
        [
         "AC",
         "Acre"
        ],
        [
         "AM",
         "Amazonas"
        ],
        [
         "RR",
         "Roraima"
        ],
        [
         "PA",
         "Pará"
        ],
        [
         "AP",
         "Amapá"
        ],
        [
         "TO",
         "Tocantins"
        ],
        [
         "MA",
         "Maranhão"
        ],
        [
         "PI",
         "Piauí"
        ],
        [
         "CE",
         "Ceará"
        ],
        [
         "RN",
         "Rio Grande do Norte"
        ],
        [
         "PB",
         "Paraíba"
        ],
        [
         "PE",
         "Pernambuco"
        ],
        [
         "AL",
         "Alagoas"
        ],
        [
         "SE",
         "Sergipe"
        ],
        [
         "BA",
         "Bahia"
        ],
        [
         "MG",
         "Minas Gerais"
        ],
        [
         "ES",
         "Espírito Santo"
        ],
        [
         "RJ",
         "Rio de Janeiro"
        ],
        [
         "SP",
         "São Paulo"
        ],
        [
         "PR",
         "Paraná"
        ],
        [
         "SC",
         "Santa Catarina"
        ],
        [
         "RS",
         "Rio Grande do Sul"
        ],
        [
         "MS",
         "Mato Grosso do Sul"
        ],
        [
         "MT",
         "Mato Grosso"
        ],
        [
         "GO",
         "Goiás"
        ],
        [
         "DF",
         "Distrito Federal"
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "sigla",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "nome",
            "nullable": true,
            "type": "string"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 12
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "sigla",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "nome",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "select * from uf_bruto;"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8973787451499892,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "nb_bronze_uf",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}