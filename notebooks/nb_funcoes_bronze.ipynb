{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02ba4a67-91ef-4cf0-bc94-e0a12f383289",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\n- requests\\n    Para as chamadas das APIs\\n- time\\n     Para pausas entre as chamadas, ajuda a evitar bloqueios\\n- ThreadPoolExecutor\\n    Para chamadas paralelas, aqui é usado para diminuir o tempo de execução na busca dos CNPJs listados\\n- pyspark.sql.functions\\n    Para manipulação de dados em SQL\\n\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Biblotecas necessárias\n",
    "\n",
    "import requests \n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed \n",
    "import pyspark.sql.functions as F \n",
    "from pyspark.sql import DataFrame \n",
    "\n",
    "\"\"\"\n",
    "- requests\n",
    "    Para as chamadas das APIs\n",
    "- time\n",
    "     Para pausas entre as chamadas, ajuda a evitar bloqueios\n",
    "- ThreadPoolExecutor\n",
    "    Para chamadas paralelas, aqui é usado para diminuir o tempo de execução na busca dos CNPJs listados\n",
    "- pyspark.sql.functions\n",
    "    Para manipulação de dados em SQL\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e315903a-e778-4007-b9da-fec4efa1a3e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\n   Faz o download de um arquivo csv de uma URL e salva em um arquivo local\\n   local_path: Caminho local onde o arquivo será salvo, será informado no arquivo principal\\n   url: URL da API em string\\n    \\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://dados.cvm.gov.br/dados/FI/CAD/DADOS/cad_fi.csv\"\n",
    "local_path = \"/Volumes/projeto_triller/bd_triller/arquivos_projeto/cad_fi.csv\"\n",
    "\n",
    "def download_csv(url: str, local_path: str) -> str:\n",
    "    response = requests.get(url, stream=True)\n",
    "    if response.status_code == 200:\n",
    "        with open(local_path, \"wb\") as file:   # salva direto no Volume\n",
    "            file.write(response.content)\n",
    "        print(f\"Arquivo salvo em: {local_path}\")\n",
    "        return local_path\n",
    "    else:\n",
    "        raise Exception(f\"Erro ao baixar arquivo: {response.status_code}\")\n",
    "\n",
    "\"\"\"\n",
    "   Faz o download de um arquivo csv de uma URL e salva em um arquivo local\n",
    "   local_path: Caminho local onde o arquivo será salvo, será informado no arquivo principal\n",
    "   url: URL da API em string\n",
    "    \n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57c91ac4-4af8-4b88-8d6f-d1d4af12b249",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nCarrega o arquivo csv em um dataframe\\nspark: SparkSession\\nfile_path: Caminho do arquivo csv\\nsep: Separador do arquivo csv\\nheader: Se o arquivo csv tem cabeçalho\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def carrega_dados(spark, file_path: str, sep: str = \";\", header: bool = True, inferSchema: bool = True) -> DataFrame:\n",
    "    return (\n",
    "        spark.read.option(\"sep\", sep)\n",
    "        .option(\"header\", header)\n",
    "        .option(\"inferSchema\", inferSchema)\n",
    "        .csv(file_path)\n",
    "    )\n",
    "\n",
    "\"\"\"\n",
    "Carrega o arquivo csv em um dataframe\n",
    "spark: SparkSession\n",
    "file_path: Caminho do arquivo csv\n",
    "sep: Separador do arquivo csv\n",
    "header: Se o arquivo csv tem cabeçalho\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "323473c8-e0aa-4a35-81b1-ad2fc932ab4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nPadroniza os CNPJs, removendo os caracteres que não sejam números.\\nA APIBRASIL aceita somente chamada com CNPJ em números\\ndf: Dataframe com os CNPJs\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def trata_cnpj(df: DataFrame)-> DataFrame:\n",
    "    colunas = [\"CNPJ_FUNDO\", \"CNPJ\"]\n",
    "    for col in df.columns:\n",
    "        df = df.withColumn(col, F.regexp_replace(F.col(col), \"[^0-9]\", \"\")) \n",
    "\n",
    "    return df\n",
    "\n",
    "\"\"\"\n",
    "Padroniza os CNPJs, removendo os caracteres que não sejam números.\n",
    "A APIBRASIL aceita somente chamada com CNPJ em números\n",
    "df: Dataframe com os CNPJs\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e489e0e-396b-4fef-9638-eb1e4996d44e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nGarante que a tabela seja removida e cria uma nova com os dados no Hive Metastore\\ndf: Dataframe com os dados\\ntabela: Nome da tabela no Hive Metastore, será informada no notebook principal\\nmodo: Modo de escrita no Hive Metastore, nesse caso (overwrite) sobrescreve se a tabela já existir\\n\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def salva_dados(df: DataFrame, tabela: str, modo: str = \"overwrite\") -> None:\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS {tabela}\")\n",
    "    df.write.mode(modo).saveAsTable(tabela)\n",
    "\n",
    "\"\"\"\n",
    "Garante que a tabela seja removida e cria uma nova com os dados no Hive Metastore\n",
    "df: Dataframe com os dados\n",
    "tabela: Nome da tabela no Hive Metastore, será informada no notebook principal\n",
    "modo: Modo de escrita no Hive Metastore, nesse caso (overwrite) sobrescreve se a tabela já existir\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "046c0ed7-6356-4ea0-af12-6cdd9a815ed9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nConsulta dados de um CNPJ em uma API, nesse caso a APIBRASIL\\nCNPJ já deve estar padronizado, sem caracteres especiais, conforme listagem no notebook\\nSe sucesso, retorna as informações principais\\nSe erro, retorna o CNPJ com a informação de erro\\nFoi incluída uma \"pausa\" para evitar bloqueios, pois são muitos CNPJs\\n\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def consulta_cnpj(cnpj: str, pausa: float = 0.5) -> dict:\n",
    "    time.sleep(pausa) #Essa pausa evita bloqueios por limite de requisições, o time utilizado é listado na linha acima\n",
    "    url = f\"https://brasilapi.com.br/api/cnpj/v1/{cnpj}\"\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            return {\n",
    "                \"razao_social\": data.get(\"nome\"),\n",
    "                \"cep\": data.get(\"cep\"),\n",
    "                \"uf\": data.get(\"uf\"),\n",
    "                \"cnpj\": data.get(\"cnpj\"),\n",
    "                \"erro\": None\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                \"razao_social\": None,\n",
    "                \"cep\": None,\n",
    "                \"uf\": None,\n",
    "                \"cnpj\": None,\n",
    "                \"erro\": f\"Erro ao consultar CNPJ: {response.status_code}\"\n",
    "            }\n",
    "    except Exception as e:\n",
    "        return { \n",
    "                \"razao_social\": None,\n",
    "                \"cep\": None,\n",
    "                \"uf\": None,\n",
    "                \"cnpj\": None,\n",
    "                \"erro\": f\"Erro ao consultar CNPJ: {str(e)}\"\n",
    "        }\n",
    "\n",
    "\"\"\"\n",
    "Consulta dados de um CNPJ em uma API, nesse caso a APIBRASIL\n",
    "CNPJ já deve estar padronizado, sem caracteres especiais, conforme listagem no notebook\n",
    "Se sucesso, retorna as informações principais\n",
    "Se erro, retorna o CNPJ com a informação de erro\n",
    "Foi incluída uma \"pausa\" para evitar bloqueios, pois são muitos CNPJs\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a82447d-42e9-425e-8aed-f4f0284e411f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nConsulta os CNPJs listados de forma paralela\\nUsa o ThreadPoolExecutor para chamar a API de forma paralela, evitando demora no retorno dos dados\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def consulta_lista_cnpjs(lista_cnpjs: list, max_workers: int = 5, pausa: float = 0.5) -> list:\n",
    "    resultado = []\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = [executor.submit(consulta_cnpj, cnpj, pausa) for cnpj in lista_cnpjs]\n",
    "        for future in as_completed(futures):\n",
    "            resultado.append(future.result())\n",
    "\n",
    "    return resultado\n",
    "\n",
    "\"\"\"\n",
    "Consulta os CNPJs listados de forma paralela\n",
    "Usa o ThreadPoolExecutor para chamar a API de forma paralela, evitando demora no retorno dos dados\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7174cda5-69be-4636-a48a-2c4070ca2135",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nPipeline para consultar os CNPJs e salvar no Hive Metastore\\nPega os CNPJs distintos da tabela de origem e salva no Hive Metastore\\ntabela_origem: Tabela de origem informada no notebook principal\\ntabela_destino: Tabela de destino informada no notebook principal\\nmax_workers: Número de threads para consulta paralela\\n\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pipeline_cnpj_brasilapi(spark, tabela_origem: str, tabela_destino: str, max_workers: int = 5, pausa: float = 0.5) -> DataFrame:\n",
    "    \n",
    "    cnpjs = spark.sql(f\"\"\"\n",
    "            SELECT DISTINCT CNPJ_FUNDO AS documento\n",
    "            FROM {tabela_origem}\n",
    "            WHERE SIT = 'EM FUNCIONAMENTO NORMAL'\n",
    "            \"\"\")\n",
    "    \n",
    "    lista_cnpjs = [row.documento for row in cnpjs.collect()]\n",
    "    dados = consulta_lista_cnpjs(lista_cnpjs, max_workers=max_workers, pausa=pausa)\n",
    "    df = spark.createDataFrame(dados)\n",
    "    df = trata_cnpj(df)\n",
    "\n",
    "    salva_dados(df, tabela_destino, \"overwrite\")\n",
    "    return df\n",
    "\n",
    "\"\"\"\n",
    "Pipeline para consultar os CNPJs e salvar no Hive Metastore\n",
    "Pega os CNPJs distintos da tabela de origem e salva no Hive Metastore\n",
    "tabela_origem: Tabela de origem informada no notebook principal\n",
    "tabela_destino: Tabela de destino informada no notebook principal\n",
    "max_workers: Número de threads para consulta paralela\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4d43412-fc87-41c1-b547-38142ba18b11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nConsulta a lista de estados do IBGE na BRasilAPI\\nRetorna a sigla e o nome de cada estado\\nSe erro, retorna o estado com a informação de erro\\n\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def consulta_uf() -> list:\n",
    "    url = 'https://brasilapi.com.br/api/ibge/uf/v1/'\n",
    "    try:\n",
    "        response = requests.get(urk, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            estados = response.json()\n",
    "            return[{\n",
    "                \"uf\": e.get(\"sigla\"),\n",
    "                \"nome\": e.get(\"nome\"),\n",
    "                \"erro\": None\n",
    "            } for e in estados]\n",
    "        else:\n",
    "            return [{\n",
    "                \"uf\": None,\n",
    "                \"nome\": None,\n",
    "                \"erro\": f\"Erro ao consultar estados: {response.status_code}\"\n",
    "            }]\n",
    "    except Exception as e:\n",
    "        return [{\n",
    "            \"uf\": None,\n",
    "            \"nome\": None,\n",
    "            \"erro\": f\"Erro ao consultar estados: {str(e)}\"\n",
    "        }]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Consulta a lista de estados do IBGE na BRasilAPI\n",
    "Retorna a sigla e o nome de cada estado\n",
    "Se erro, retorna o estado com a informação de erro\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47ec7975-e071-46d0-b7a3-998b85eb4955",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nPipeline completo para UFs\\nConsulta a API\\nCria o DataFrame e salva em uma tabela de destino informada no notebook principal\\n\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pipeline_ufs(spark, tabela_destino: str) -> DataFrame:\n",
    "    dados = consulta_uf()\n",
    "    df = spark.createDataFrame(dados)    \n",
    "    salva_dados(df, tabela_destino, \"overwrite\")\n",
    "    return df\n",
    "\n",
    "\"\"\"\n",
    "Pipeline completo para UFs\n",
    "Consulta a API\n",
    "Cria o DataFrame e salva em uma tabela de destino informada no notebook principal\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "nb_funcoes_bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}